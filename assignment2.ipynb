{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Question 1]\n",
        "\n",
        "Derive the update rule and show how to train a 2-layer (1 hidden layer and 1 output layer) neural\n",
        "network with backpropagation for regression using the Mean Square Error loss. Assume that you\n",
        "are using the Sigmoid activation function for the hidden layer. Explain briefly how this is different\n",
        "from the update rule for the network trained for binary classification using log loss."
      ],
      "metadata": {
        "id": "GHu8rOmA-BET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary cross entropy (also known as logarithmic loss or log loss) is a model metric that tracks incorrect labeling of the data class by a model, penalizing the model if deviations in probability occur into classifying the labels. Low log loss values equate to high accuracy values. Binary cross entropy is equal to -1*log(likelihood). Here Yi represents the actual class and log(p(yi)is the probability of that class. p(yi) is the probability of one 1-p(yi) is the probability of zero. Log loss can be used in training as the logistic regression cost function and in production as a performance metric for binary classification. This piece focuses on how to leverage log loss in a production setting.**\n",
        "\n",
        "**While in backpropagation, the input signals are x1,x2,x3,x4 .. xn and their weights are w1,w2,w3,w4 .. wn. Then the node has an activation function which is the summation of all the inputs and weights with their output being y = F(xiwi)**\n",
        "\n",
        "**Back propopagtion algorithm**\n",
        "**(backward propagation of error)**\n",
        "**when error occurs we go in backward direction.**\n",
        "**output -> hidden -> iput layers**\n",
        "**There is a bias factor between input and hidden layers and then alos between hidden and output layers.**"
      ],
      "metadata": {
        "id": "w9o2cYyUBjrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 2]\n",
        "\n",
        "\n",
        "For the given data on canvas, construct a neural network for the regression task. **Your network must have 1 hidden layer and 1 output layer.** **Use sigmoid to be your activation function for the hidden layer(s).** You can choose the number of neurons in each layer using your intuition.\n",
        "\n",
        "\n",
        "The data is already split to have your input data for training (X_train.csv) and testing (X_train.csv) and their corresponding target values Y_train.csv and Y_test.csv, respectively. You can load the data using NumPy as follows:\n",
        "\n",
        "X_train = np.loadtxt(\"X_train.csv\")\n",
        "\n",
        "Implement the backpropagation algorithm and train your network until convergence.\n"
      ],
      "metadata": {
        "id": "XehyXiug-I9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WcleeyHgtAjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.loadtxt(\"X_train.csv\")\n",
        "Y = np.loadtxt(\"Y_train.csv\")"
      ],
      "metadata": {
        "id": "lPkKOMHVtEjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For one example $x^{(i)}$:\n",
        "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$\n",
        "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
        "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
        "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "Given the predictions on all the examples, you can also compute the cost $J$ as follows:\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "The general methodology to build a Neural Network is to:\n",
        "    1. Define the neural network structure ( # of input units,  # of hidden units, etc).\n",
        "    2. Initialize the model's parameters\n",
        "    3. Loop:\n",
        "        - Implement forward propagation\n",
        "        - Compute loss\n",
        "        - Implement backward propagation to get the gradients\n",
        "        - Update parameters (gradient descent)"
      ],
      "metadata": {
        "id": "3FK7NRsktj33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: layer_sizes\n",
        "\n",
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    n_x = X.shape[1] # size of input layer\n",
        "    n_h = 1\n",
        "    n_y = Y.shape[1] # size of output layer\n",
        "    return (n_x, n_h, n_y)"
      ],
      "metadata": {
        "id": "oP-0MfCZtrzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_assess, Y_assess = layer_sizes_test_case()\n",
        "(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n",
        "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
        "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
        "print(\"The size of the output layer is: n_y = \" + str(n_y))"
      ],
      "metadata": {
        "id": "5LIxe4kEt0iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "\n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
        "\n",
        "    W1 = np.random.randn(n_h,n_x)* 0.01\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "8y9GAULat3wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_x, n_h, n_y = initialize_parameters_test_case()\n",
        "\n",
        "parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "metadata": {
        "id": "_cCBOnyEt_Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**W1**\n",
        "    [[-0.00416758 -0.00056267]\n",
        " [-0.02136196  0.01640271]\n",
        " [-0.01793436 -0.00841747]\n",
        " [ 0.00502881 -0.01245288]]\n",
        "  \n",
        "  \n",
        "\n",
        "  b1\n",
        " [[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]\n",
        "  \n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2**</td>\n",
        "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td>\n",
        "  </tr>\n",
        "  \n",
        "\n",
        "\n",
        "  <tr>\n",
        "    <td>**b2**</td>\n",
        "    <td> [[ 0.]] </td>\n",
        "  </tr>"
      ],
      "metadata": {
        "id": "ayXz_LXfuBxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement `forward_propagation()`.\n",
        "\n",
        "\n",
        "sigmoid binary classifer only used for output layer slow grad descent\n",
        "tanh normalized -> gradient descent is faster\n",
        "Relu default choice for activation slope = 1/0\n",
        "\n"
      ],
      "metadata": {
        "id": "vEk6Rj8luj7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "\n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    Z1 = np.dot(W1,X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2,A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "\n",
        "    return A2, cache"
      ],
      "metadata": {
        "id": "9jX8iID7ujrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_assess, parameters = forward_propagation_test_case()\n",
        "\n",
        "A2, cache = forward_propagation(X_assess, parameters)\n",
        "\n",
        "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
      ],
      "metadata": {
        "id": "TYz85brSuv6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output\n",
        "-0.000499755777742 -0.000496963353232 0.000438187450959 0.500109546852"
      ],
      "metadata": {
        "id": "ksfIedxZuy9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: compute_cost\n",
        "\n",
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "\n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1] # number of example\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "\n",
        "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1 - A2),1 - Y)\n",
        "    cost = - np.sum(logprobs) * (1 / m)\n",
        "\n",
        "\n",
        "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect.\n",
        "                                # E.g., turns [[17]] into 17\n",
        "    assert(isinstance(cost, float))\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "EHIoF_tvu9K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A2, Y_assess, parameters = compute_cost_test_case()\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
      ],
      "metadata": {
        "id": "IZdnP1oTvB_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cost 0.692919893776"
      ],
      "metadata": {
        "id": "cgt4nGSQvDte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "\n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    A1 = cache[\"A1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "\n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2.\n",
        "    dZ2= A2 - Y\n",
        "    dW2 = 1 / m *(np.dot(dZ2,A1.T))\n",
        "    db2 = 1 / m *(np.sum(dZ2,axis = 1,keepdims = True))\n",
        "    dZ1 = np.dot(W2.T,dZ2) * (1 - np.power(A1, 2))\n",
        "    dW1 = 1 / m *(np.dot(dZ1,X.T))\n",
        "    db1 = 1 / m *(np.sum(dZ1,axis = 1,keepdims = True))\n",
        "\n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "gyX4eKdUvMcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
        "\n",
        "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
        "print (\"db2 = \"+ str(grads[\"db2\"]))"
      ],
      "metadata": {
        "id": "waPrkTzbvW1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dW1 = [[ 0.01018708 -0.00708701]\n",
        " [ 0.00873447 -0.0060768 ]\n",
        " [-0.00530847  0.00369379]\n",
        " [-0.02206365  0.01535126]]\n",
        "\n",
        "db1 = [[-0.00069728]\n",
        " [-0.00060606]\n",
        " [ 0.000364  ]\n",
        " [ 0.00151207]]\n",
        "\n",
        "dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]\n",
        "\n",
        "db2 = [[ 0.06589489]]"
      ],
      "metadata": {
        "id": "WeD5f496vaDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "ZzPp13Gnvs6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads)\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "metadata": {
        "id": "xOyxOOwDvs1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W1 = [[-0.00643025  0.01936718]\n",
        " [-0.02410458  0.03978052]\n",
        " [-0.01653973 -0.02096177]\n",
        " [ 0.01046864 -0.05990141]]\n",
        "\n",
        "b1 = [[ -1.02420756e-06]\n",
        " [  1.27373948e-05]\n",
        " [  8.32996807e-07]\n",
        " [ -3.20136836e-06]]\n",
        "\n",
        "W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\n",
        "\n",
        "b2 = [[ 0.00010457]]"
      ],
      "metadata": {
        "id": "pzsHUL95v0Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: nn_model\n",
        "\n",
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "\n",
        "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
        "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "# Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X,parameters)\n",
        "\n",
        "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = compute_cost(A2, Y, parameters)\n",
        "\n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "\n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = update_parameters(parameters, grads)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "Z_97vqNivswW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_assess, Y_assess = nn_model_test_case()\n",
        "\n",
        "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "metadata": {
        "id": "qTgOUcg7vsqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W1 = [[-4.18494056  5.33220609]\n",
        " [-7.52989382  1.24306181]\n",
        " [-4.1929459   5.32632331]\n",
        " [ 7.52983719 -1.24309422]]\n",
        "\n",
        "b1 = [[ 2.32926819]\n",
        " [ 3.79458998]\n",
        " [ 2.33002577]\n",
        " [-3.79468846]]\n",
        "\n",
        "W2 = [[-6033.83672146 -6008.12980822 -6033.10095287  6008.06637269]]\n",
        "\n",
        "b2 = [[-52.66607724]]"
      ],
      "metadata": {
        "id": "-v15-LDNwA1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    X -- input data of size (n_x, m)\n",
        "\n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    A2, cache = forward_propagation(X,parameters)\n",
        "    predictions = (A2 > 0.5)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "rGqRLgXovsk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, X_assess = predict_test_case()\n",
        "\n",
        "predictions = predict(parameters, X_assess)\n",
        "print(\"predictions mean = \" + str(np.mean(predictions)))"
      ],
      "metadata": {
        "id": "IqIpMRCpvseE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predictions mean = 0.666666666667"
      ],
      "metadata": {
        "id": "LptqxOKvwNhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
      ],
      "metadata": {
        "id": "cKHtPzlEwNS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cost for 9000 iterations 0.218607"
      ],
      "metadata": {
        "id": "ll--c58SwY8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "metadata": {
        "id": "3wUglFFSvsWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy 90%"
      ],
      "metadata": {
        "id": "Un-kxqkwwhEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
        "for i, n_h in enumerate(hidden_layer_sizes):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer of size %d' % n_h)\n",
        "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
        "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "    predictions = predict(parameters, X)\n",
        "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
      ],
      "metadata": {
        "id": "eNWXzIB3vsKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bL-Q52r6vr78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the following questions:\n",
        "1. What is the activation function that you will choose for the output layer? Justify your answer\n",
        "briefly.\n",
        "**sigmoid as my activation function for the output. What was taught in class**\n",
        "\n",
        "2. How many neurons should there be in the output layer? Why?\n",
        "**One because there are only one hidden layer and that becomes the input for the next layer.**\n",
        "\n",
        "3. Report the average MSE loss and the accuracy.\n",
        "**0.692919893776 MSE loss**\n",
        "**90% accuracy**\n",
        "\n",
        "\n",
        "4. Plot the loss and accuracy as a function of the number of iterations.\n",
        "**done at bottom of code**\n",
        "\n",
        "5. What is the effect of the learning rate on the training process? Vary the learning rate to be between 0.001 and 1.0 and plot the resulting accuracy as a function of learning rate.\n",
        "**the larger the learning rate gets the more decay or slower learning of the function**\n",
        "\n",
        "6. What is the effect of the number of neurons in the hidden layer? To answer this question, you will need to consider and answer the following:\n",
        "**As the number increases with neurons and hidden layers the performance gets better.**\n",
        "\n",
        "  a. You will need to vary the number of neurons from 1 to 10. Does the update rule need to be changed/derived again? Why or why not?\n",
        "**It means incrementing the output and adding another weight.No it doesn't change it just expands and add another weight**\n",
        "\n",
        "  b. Report your observations by reporting the final loss and plotting the true labels and your predicted labels, along with a brief (2-3 lines) description.\n",
        "\n",
        "  **Accuracy for 1 hidden units: 67.5 %\n",
        "Accuracy for 2 hidden units: 67.25 %\n",
        "Accuracy for 3 hidden units: 90.75 %\n",
        "Accuracy for 4 hidden units: 90.5 %\n",
        "Accuracy for 5 hidden units: 91.25 %**\n",
        "**The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. **\n",
        "** The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to  fits the data well without also incurring noticable overfitting**\n",
        "\n",
        "7. What is the effect of the activation functions in the network? Explore two different activation functions other than sigmoid such as tanh, linear, or ReLU.\n",
        "tanh and Relu used\n",
        "\n",
        "  a. Will you need to change the update rule?**no you can use the same update rule for tanh.**\n",
        "\n",
        "  b. What is the change that you need to make to achieve this experiment?\n",
        "  **the output as tanh uses a range of [-1, 1] and ReLu is [0, infinity]**\n",
        "\n",
        "  c. Report your observations by reporting the final loss and plotting the true labels and your predicted labels, along with a brief (2-3 lines) description\n",
        "\n",
        "  **gradient update rule.**\n",
        "  **forward propogation used**\n",
        "  **final loss is 0.6929198937**\n",
        "  **predictied labels are 47%**\n",
        "  **prediction is 0.666666666667**"
      ],
      "metadata": {
        "id": "Vi7-QPkm-ecN"
      }
    }
  ]
}