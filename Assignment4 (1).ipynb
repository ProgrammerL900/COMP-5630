{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 1**##\n",
        "\n",
        "Question 1: Naïve Bayes Classifier\n",
        "\n",
        "\n",
        "\n",
        "Part 1: Conditional Probability Distribution Calculation\n",
        "\n",
        "\n",
        "\n",
        "Assuming the dataset contains categorical features, and the goal is to predict whether a house is above or below a certain price (binary classification), here is how we would manually compute the conditional probability distribution for each feature\n",
        "\n",
        "\n",
        "\n",
        "**Calculate the Frequency Counts:** For each feature within each class (above/below the price threshold), count how many times each category occurs.\n",
        "\n",
        "**Compute Conditional Probabilities:** Divide the frequency of each category by the total number of instances in that class to get the probability of each category given the class.\n",
        "\n",
        "Example features might include \"Number of Bathrooms\" and \"Age of Home\". Here’s a detailed explanation for calculating probabilities for \"Number of Bathrooms\":\n",
        "\n",
        "\n",
        "\n",
        "Feature: Number of Bathrooms\n",
        "\n",
        "Categories: 1, 2, 3\n",
        "\n",
        "Suppose we have data that looks like this for a binary classification:\n",
        "\n",
        "Class \"Above\": $6807\n",
        "\n",
        "1 Bathroom: 1\n",
        "\n",
        "1.5 Bathrooms: 2\n",
        "\n",
        "2.5 bathrooms: 2\n",
        "\n",
        "Total in \"Above\" class = 5\n",
        "\n",
        "Class \"Below\": $6807 (From averaging all the local prices to find a median. It would be harder to do an average within each bathroom type because 2.5 bathrooms only has two numbers which wouldn't give a good average as 1 bathroom that has 10+ prices for a bigger average range)\n",
        "\n",
        "1 Bathroom: 14\n",
        "\n",
        "1.5 Bathrooms: 1\n",
        "\n",
        "2.5 bathrooms: 0\n",
        "\n",
        "Total in \"Below\" class = 15\n",
        "\n",
        "Conditional Probabilities:\n",
        "\n",
        "P(Bathrooms=1 | Above) = 1/5 = 0.2\n",
        "\n",
        "P(Bathrooms=2.5 | Above) = 2/5 = 0.4\n",
        "\n",
        "P(Bathrooms=1.5 | Above) = 2/5 = 0.4\n",
        "\n",
        "P(Bathrooms=1 | Below) = 14/15 = 0.933\n",
        "\n",
        "P(Bathrooms=2.5 | Below) = 0/15 = 0\n",
        "\n",
        "P(Bathrooms=1.5 | Below) = 1/15 ≈ 0.066"
      ],
      "metadata": {
        "id": "Fw0iS8rIR2HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature garages\n",
        "average: 1.2 garages round down to 1.\n",
        "\n",
        "above:\n",
        "\n",
        "0 garages: 17\n",
        "\n",
        "1 garages: 8\n",
        "\n",
        "1.5 garages: 6\n",
        "\n",
        "2 garages: 0\n",
        "\n",
        "total: 31\n",
        "\n",
        "below:\n",
        "0 garages: 0\n",
        "\n",
        "1 garages: 3\n",
        "\n",
        "1.5 garages: 9\n",
        "\n",
        "2 garages: 14\n",
        "\n",
        "total: 26\n",
        "\n",
        "P(Garages=0 | Above) = 17/31 = 0.548\n",
        "\n",
        "P(Garages=1 | Above) = 8/31 = 2.580\n",
        "\n",
        "P(Garages=1.5 | Above) = 6/31 = 0.193\n",
        "\n",
        "P(Garages=2 | Above) = 0/31 = 0\n",
        "\n",
        "P(Garages=0 | Below) = 0/26 = 0\n",
        "\n",
        "P(Garages=1 | Below) = 3/26 = 0.115\n",
        "\n",
        "P(Garages=1.5 | Below) = 9/26 = 0.346\n",
        "\n",
        "P(Garages=2 | Below) = 14/26 = 0.538\n"
      ],
      "metadata": {
        "id": "FrCwTjGPXgh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the conditional probabilities as obtained manually\n",
        "probabilities = {\n",
        "    'Bathrooms': {\n",
        "        'Above': {1: 0.20, 2.5: 0.40, 1.5: 0.40},\n",
        "        'Below': {1: 0.93, 2.5: 0.00, 1.5: 0.06}\n",
        "    },\n",
        "    'Garages' : {\n",
        "       'Above': {0: 0.54, 1: 0.258, 1.5: 0.19, 2: 0.00},\n",
        "       'Below' : {0: 0.00, 1: 0.11, 1.5: 0.34, 2: 0.53}\n",
        "    }\n",
        "    # Add other features following the same structure\n",
        "}\n",
        "\n",
        "# Prior probabilities\n",
        "prior_above = 0.5  # Placeholder: replace with actual data\n",
        "prior_below = 0.5  # Placeholder: replace with actual data\n",
        "\n",
        "# Function to classify a new example using the MAP rule\n",
        "def classify(example):\n",
        "    prob_above = prior_above\n",
        "    prob_below = prior_below\n",
        "    for feature, value in example.items():\n",
        "        prob_above *= probabilities[feature]['Above'].get(value, 0)\n",
        "        prob_below *= probabilities[feature]['Below'].get(value, 0)\n",
        "\n",
        "    # Compare probabilities and classify\n",
        "    if prob_above > prob_below:\n",
        "        return 'Above'\n",
        "    else:\n",
        "        return 'Below'\n",
        "\n",
        "# Example usage:\n",
        "test_example = {'Bathrooms': 2.5}\n",
        "print(classify(test_example))"
      ],
      "metadata": {
        "id": "k4c5U2lNWXHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 2**#\n",
        "Using the same housing data (Asssignment4_Data.xlsx), construct a decision tree classifier. You can use the implementation available on Sci-Kit Learn. Perform the following experiments and\n",
        "briefly (2-4 sentences) answer the questions.\n",
        "1. Use the default parameters.\n",
        "\n",
        "a. What is the accuracy on the training set?\n",
        "0.446\n",
        "\n",
        "b. What is the accuracy on the test set?\n",
        "0.764\n",
        "\n",
        "2. What is the effect of restricting the maximum depth of the tree? Try different depths and find the best value.\n",
        "As depth increases, training set prediction error(means accuracy) gets better while validation set prediction error(means accuracy) gets worse.\n",
        "Combats overfitting by making smaller trees\n",
        "\n",
        "3. Why does restricting the depth have such a strong effect on the classifier performance?\n",
        " When you restrict the depth of the tree, you limit its complexity, reducing the risk of overfitting. This can lead to better generalization to new data (test set), but if the depth is too restricted, the model may underfit, leading to poor performance on both training and test data.\n",
        "\n",
        "4. Visualize the resulting tree. Perform the inference on this tree manually (i.e. show/trace the path taken towards classification) and provide a classification for the followinge xample:\n",
        "\n",
        "Local Price 9.0384\n",
        "Bathrooms 1\n",
        "Land Area 7.8\n",
        "Living area 1.5\n",
        "Garages 1.5\n",
        "Rooms 7\n",
        "Bedrooms 3\n",
        "\n",
        "\n",
        "\n",
        "**Default Parameters:**\n",
        "\n",
        "Train a decision tree on the training set.\n",
        "\n",
        "Evaluate its accuracy on both the training and test sets.\n",
        "\n",
        "**Maximum Depth of the Tree:**\n",
        "\n",
        "Train multiple decision trees with varying maximum depths.\n",
        "\n",
        "Analyze how the depth affects the accuracy and find the best depth.\n",
        "\n",
        "**Reason for Depth Effect:**\n",
        "\n",
        "Explain why limiting the depth impacts the performance.\n",
        "\n",
        "**Manual Inference on a Decision Tree:**\n",
        "\n",
        "Visualize the tree.\n",
        "\n",
        "Manually trace the path for a given example to classify it.\n",
        "\n"
      ],
      "metadata": {
        "id": "eD8IpJA6dE3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# load the data from the excel file\n",
        "file_path = 'Assignment4_Data.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Hypothetical training and test data loaded into X_train, y_train, X_test, y_test\n",
        "\n",
        "# Part 1: Use default parameters\n",
        "tree_default = DecisionTreeClassifier()\n",
        "tree_default.fit(X_train, y_train)\n",
        "print(\"Training Accuracy:\", accuracy_score(y_train, tree_default.predict(X_train)))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, tree_default.predict(X_test)))\n",
        "\n",
        "# Part 2: Effect of restricting the maximum depth\n",
        "depths = range(1, 11)  # Depths from 1 to 10\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "for depth in depths:\n",
        "    tree = DecisionTreeClassifier(max_depth=depth)\n",
        "    tree.fit(X_train, y_train)\n",
        "    train_acc.append(accuracy_score(y_train, tree.predict(X_train)))\n",
        "    test_acc.append(accuracy_score(y_test, tree.predict(X_test)))\n",
        "\n",
        "plt.plot(depths, train_acc, label='Training Accuracy')\n",
        "plt.plot(depths, test_acc, label='Test Accuracy')\n",
        "plt.xlabel('Depth of tree')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Part 3: Explanation\n",
        "# When you restrict the depth of the tree, you limit its complexity, reducing the risk of overfitting. This can lead to better generalization to new data (test set), but if the depth is too restricted, the model may underfit, leading to poor performance on both training and test data.\n",
        "\n",
        "# Part 4: Visualize and manually classify an example\n",
        "tree_best = DecisionTreeClassifier(max_depth=3)  # Using an optimal depth from above results\n",
        "tree_best.fit(X_train, y_train)\n",
        "plot_tree(tree_best, filled=True)\n",
        "plt.show()\n",
        "\n",
        "# For manual inference, use the plot to follow the path based on feature values of the example."
      ],
      "metadata": {
        "id": "zLSs4bQWdLQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3**#\n",
        "Using the same housing data (Asssignment4_Data.xlsx), implement the k-nearest neighbor algorithm to perform classification. Your program should take in the number of neighbors k as input and classify each example in the test set based on the majority vote from the chosen neighbors. Compute the accuracy of your approach for different number of neighbors, ranging from 1 to 5 and explain the results briefly using a plot. You can use Euclidean distance to choose\n",
        "the neighbor points"
      ],
      "metadata": {
        "id": "ZdI1ogwAdLkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# load the data from the excel file\n",
        "file_path = 'Assignment4_Data.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "k_values = range(1, 6)\n",
        "accuracies = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, knn.predict(X_test))\n",
        "    accuracies.append(acc)\n",
        "\n",
        "plt.plot(k_values, accuracies, marker='o')\n",
        "plt.xlabel('k value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('k-NN Varying k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3BmWFZnTcn6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}