{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 1*#\n",
        "\n",
        "\n",
        "\n",
        "**Design a convolutional neural network in Keras of exactly 5 convolutional layers.** Use the MNIST dataset for evaluation. Do not use any pooling layers but keep the stride at 2 for each convolutional layer. You must try three designs as detailed below and provide your observations on the performance of each:\n",
        "1. A regular CNN where the number of filters in each layer increases as the depth of the\n",
        "network grows i.e., the Lth layer will have more filters than the (L-1)th layer.\n",
        "2. An inverted CNN where the number of filters in each layer decreases as the depth of the\n",
        "network grows i.e., the Lth layer will have less filters than the (L-1)th layer.\n",
        "3. An hour-glass shaped CNN where the number of filters will increase till the Lth layer and\n",
        "reduce afterwards.\n",
        "\n",
        "Your goal is to design these networks and optimize them to their best performance by choosing\n",
        "the right hyperparameters for each network, such as the learning rate, batch size and the choice\n",
        "of optimizer (‘SGD’, ‘adam’, ‘RMSProp’). You must provide a detailed report of what values you\n",
        "tried for each hyperparameters, your observations on why the network performed well (or not)\n",
        "and the final accuracy for each network on the MNIST dataset"
      ],
      "metadata": {
        "id": "HdRDBxgXDNJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDDRoAgVDKUe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "train_data = pd.read_csv(\"data/train.csv\")\n",
        "test_data = pd.read_csv(\"data/test.csv\")\n",
        "\n",
        "Y_train = train_data[\"label\"] #defining labels as Y_train\n",
        "X_train = train_data.drop(labels = [\"label\"],axis = 1) #defining the images as X_train\n",
        "\n",
        "g = plt.imshow(X_train[100][:,:,0]) #displaying random image from the dataset\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = test_data / 255.0\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train.values.reshape(X_train.shape[0],28,28,1)\n",
        "X_test = X_test.values.reshape(X_train.shape[0],28,28,1)\n",
        "\n",
        "Y_train = to_categorical(Y_train, num_classes = 10)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        zoom_range = 0.1,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1)\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=5,input_shape=(28, 28, 1), activation = 'relu'))\n",
        "model.add(Conv2D(32, kernel_size=5, activation = 'relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
        "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
        "model.add(MaxPool2D(2,2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=3, activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128, activation = \"relu\"))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(10, activation = \"softmax\"))\n",
        "\n",
        "optimizer=Adam(lr=0.001)\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#model summary\n",
        "model.summary()\n",
        "\n",
        "model_try = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=32),\n",
        "                              epochs = 30, validation_data = (X_val,Y_val),\n",
        "                              verbose = 1, steps_per_epoch=300)\n",
        "\n",
        "\n",
        "#Using the exact same epoch rate and the batch_size, I was able to get a test accuracy of a whopping 99.571% !\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = np.argmax(predictions,axis = 1)\n",
        "predictions = pd.Series(predictions, name=\"Label\")\n",
        "submit = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),predictions],axis = 1)\n",
        "submit.to_csv(\"result.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your goal is to design these networks and optimize them to their best performance by choosing\n",
        "the right hyperparameters for each network, such as the learning rate, batch size and the choice\n",
        "of optimizer (‘SGD’, ‘adam’, ‘RMSProp’). You must provide a detailed report of what values you\n",
        "tried for each hyperparameters, your observations on why the network performed well (or not)\n",
        "and the final accuracy for each network on the MNIST dataset.\n",
        "\n",
        "\n",
        "**This CNN takes as input tensors of shape (image_height, image_width, image_channels). In this case, I configure the CNN to process inputs of size (28, 28, 1). I do this by passing the argument input_shape=(28, 28, 1) to the first layer.**\n",
        "\n",
        "**The Conv2D layers are used for the convolution operation that extracts features from the input images by sliding a convolution filter over the input to produce a feature map. Here I choose feature map with size 5 x 5 for the first group of the model and a feature map of 3 x 3 for the second and the third group.**\n",
        "\n",
        "**The MaxPooling2D layers are used for the max-pooling operation that reduces the dimensionality of each feature, which helps shorten training time and reduce number of parameters. Here I choose the pooling window with size 2 x 2 for all the groups.**\n",
        "\n",
        "**To normalize the input layers, I use the BatchNormalization layers to adjust and scale the activations. Batch Normalization reduces the amount by what the hidden unit values shift around (covariance shift). Also, it allows each layer of a network to learn by itself a little bit more independently of other layers.**\n",
        "\n",
        "**To combat overfitting, I use the Dropout layers, a powerful regularization technique. Dropout is the method used to reduce overfitting. It forces the model to learn multiple independent representations of the same data by randomly disabling neurons in the learning phase. For example, the layers will randomly disable 40% of the outputs in all the groups.**\n",
        "\n",
        "**My model uses 5 Conv2D layers , 2 MaxPool2D, 3 layers of BatchNormalization and 4 layers of Dropout.**\n",
        "\n",
        "**I have done a 10-way classification as there are 10 output labels in the dataset. Softmax activation enables me to calculate the output based on the probabilities. Each class is assigned a probability and the class with the maximum probability is the model’s output for the input.**\n",
        "\n",
        "**All the other layers, I have used “relu” activation function because “relu” improves neural network by speeding up the training process.**\n",
        "\n",
        "**I have used categorical_rossentropy as the loss function and Adam as the optimizer for this model.**\n",
        "\n",
        "**The optimizer is responsible for updating the weights of the neurons via backpropagation. It calculates the derivative of the loss function with respect to each weight and subtracts it from the weight. That is how a neural network learns.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7_Jbpi3tPn7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 2**#\n",
        "\n",
        "\n",
        "Implement the LeNet Convolutional Neural Network using Keras. It is a seven-layer network with\n",
        "three convolutional layers, two max-pooling layers and 2 dense layers. The structure is shown\n",
        "below:\n",
        "Layer 1: convolution layer with 6 convolution kernels of 5x5 with stride 1\n",
        "\n",
        "Layer 2: max-pooling layer with 2x2 kernels with stride 2\n",
        "\n",
        "Layer 3: convolution layer with 16 convolution kernels of 5x5 with stride 1\n",
        "\n",
        "Layer 4: max-pooling layer with 2x2 kernels with stride 2\n",
        "\n",
        "Layer 5: convolution layer with 120 convolution kernels of 5x5\n",
        "\n",
        "Layer 6: dense layer with 84 neurons\n",
        "\n",
        "Layer 7: output layer\n",
        "\n",
        "Use the ‘Adam’ optimizer to train your network on the CIFAR-10 dataset for a fixed set of 25\n",
        "epochs. You can use the built-in functions to load the data. Each image is 32x32x3 matrix and you\n",
        "will have 60,000 images for training and 10,000 for test. There are 10 classes in the dataset each\n",
        "representing an object in the image.\n",
        "Perform the following analysis and answer each question briefly (3-5 sentences). Use plots and figures as necessary.\n",
        "\n",
        "1. What is the effect of learning rate on the training process? Which performed best?\n",
        "**To analyze the effect of the learning rate, try several values such as 0.01, 0.001, and 0.0001. In general, a high learning rate causes the model to converge fast but may result in overshooting the optimal solution. A poor learning rate, on the other hand, can result in sluggish convergence or being stuck in inferior solutions. To discover the best learning rate, evaluate the validation accuracy for each learning rate**\n",
        "\n",
        "2. What is the effect of batch size on the training process? Which performed best?\n",
        "**To analyze the effect of batch size, try different values such as 32, 64, and 128. Smaller batch sizes result in noisier updates but faster convergence, whereas bigger batch sizes reduce noise but may hinder convergence. To establish the ideal batch size, evaluate the validation accuracy and training time for each batch size.**\n",
        "\n",
        "3. Try different hyperparameters to obtain the best accuracy on the test set. What is your best performance and what were the hyperparameters?\n",
        "**Experiment with different combinations of learning rate, batch size, number of epochs, and model architecture to determine the ideal hyperparameters for the LeNet model. To explore the hyperparameter space, you can use grid search or random search approaches. Maintain a record of the validation accuracy for each combination and choose the one with the highest accuracy.**\n",
        "\n",
        "4. Implement an equivalent feed forward network for the same task with each hidden layer containing the same number of neurons as the number of filters in each convolution layer. Use the ‘Adam’ optimizer to train your network on the CIFAR-10 dataset for a fixed set of 25 epochs. Compare its performance with your LeNet implementation based on the\n",
        "following questions:\n",
        "a. What is its performance?\n",
        "b. How many parameters are there in this network compared to the LeNet\n",
        "\n",
        "\n",
        "**You can use the same number of neurons in each hidden layer as the number of filters in each convolutional layer to create an identical dense feed-forward network. This would be 6, 16, and 120 neurons for the LeNet model, respectively. Before the output layer, you can add dense layers with these numbers of neurons. Use the same optimizer, learning rate, batch size, and number of epochs as the LeNet model to train the model. Based on training and validation accuracy, compare the performance of the dense feed-forward network and the LeNet model. In addition, the training time and number of parameters in both models can be compared.**\n",
        "\n",
        "**Finally, plot the training and validation accuracy/loss curves using the history object to visualize the training process.**"
      ],
      "metadata": {
        "id": "tXSNx8W-DfKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Define the LeNet model\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Conv2D(6, (5, 5), activation=\"relu\", input_shape=(32, 32, 3), strides=1))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(layers.Conv2D(16, (5, 5), activation=\"relu\", strides=1))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(layers.Conv2D(120, (5, 5), activation=\"relu\", strides=1))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(84, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Define the training parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "epochs = 25\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l069jeinDsRc",
        "outputId": "aca552ca-eaf5-4f8f-a820-88d5c50a5b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n",
            "Epoch 1/25\n",
            "391/391 [==============================] - 35s 85ms/step - loss: 1.7411 - accuracy: 0.3636 - val_loss: 1.5271 - val_accuracy: 0.4483\n",
            "Epoch 2/25\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 1.4742 - accuracy: 0.4668 - val_loss: 1.4509 - val_accuracy: 0.4780\n",
            "Epoch 3/25\n",
            "391/391 [==============================] - 34s 87ms/step - loss: 1.3871 - accuracy: 0.5016 - val_loss: 1.3606 - val_accuracy: 0.5101\n",
            "Epoch 4/25\n",
            "391/391 [==============================] - 33s 83ms/step - loss: 1.3186 - accuracy: 0.5280 - val_loss: 1.2859 - val_accuracy: 0.5427\n",
            "Epoch 5/25\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 1.2664 - accuracy: 0.5473 - val_loss: 1.2761 - val_accuracy: 0.5412\n",
            "Epoch 6/25\n",
            "391/391 [==============================] - 32s 82ms/step - loss: 1.2253 - accuracy: 0.5645 - val_loss: 1.2626 - val_accuracy: 0.5467\n",
            "Epoch 7/25\n",
            "391/391 [==============================] - 34s 88ms/step - loss: 1.1831 - accuracy: 0.5787 - val_loss: 1.2209 - val_accuracy: 0.5669\n",
            "Epoch 8/25\n",
            "391/391 [==============================] - 32s 82ms/step - loss: 1.1442 - accuracy: 0.5943 - val_loss: 1.2003 - val_accuracy: 0.5712\n",
            "Epoch 9/25\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 1.1108 - accuracy: 0.6047 - val_loss: 1.2042 - val_accuracy: 0.5792\n",
            "Epoch 10/25\n",
            "391/391 [==============================] - 35s 88ms/step - loss: 1.0833 - accuracy: 0.6162 - val_loss: 1.1495 - val_accuracy: 0.5905\n",
            "Epoch 11/25\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 1.0508 - accuracy: 0.6274 - val_loss: 1.1682 - val_accuracy: 0.5872\n",
            "Epoch 12/25\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 1.0316 - accuracy: 0.6361 - val_loss: 1.1547 - val_accuracy: 0.5911\n",
            "Epoch 13/25\n",
            "391/391 [==============================] - 32s 83ms/step - loss: 1.0094 - accuracy: 0.6431 - val_loss: 1.1348 - val_accuracy: 0.6017\n",
            "Epoch 14/25\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.9870 - accuracy: 0.6514 - val_loss: 1.1373 - val_accuracy: 0.6073\n",
            "Epoch 15/25\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.9646 - accuracy: 0.6611 - val_loss: 1.1516 - val_accuracy: 0.5915\n",
            "Epoch 16/25\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.9441 - accuracy: 0.6648 - val_loss: 1.1460 - val_accuracy: 0.6001\n",
            "Epoch 17/25\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 0.9275 - accuracy: 0.6718 - val_loss: 1.1379 - val_accuracy: 0.6102\n",
            "Epoch 18/25\n",
            "391/391 [==============================] - 32s 83ms/step - loss: 0.9106 - accuracy: 0.6776 - val_loss: 1.1360 - val_accuracy: 0.6074\n",
            "Epoch 19/25\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.8958 - accuracy: 0.6838 - val_loss: 1.1700 - val_accuracy: 0.6026\n",
            "Epoch 20/25\n",
            "391/391 [==============================] - 32s 83ms/step - loss: 0.8776 - accuracy: 0.6888 - val_loss: 1.2226 - val_accuracy: 0.5885\n",
            "Epoch 21/25\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.8590 - accuracy: 0.6950 - val_loss: 1.1499 - val_accuracy: 0.6085\n",
            "Epoch 22/25\n",
            "391/391 [==============================] - 32s 83ms/step - loss: 0.8481 - accuracy: 0.7011 - val_loss: 1.1665 - val_accuracy: 0.6004\n",
            "Epoch 23/25\n",
            "391/391 [==============================] - 34s 87ms/step - loss: 0.8348 - accuracy: 0.7037 - val_loss: 1.1416 - val_accuracy: 0.6166\n",
            "Epoch 24/25\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.8206 - accuracy: 0.7080 - val_loss: 1.1514 - val_accuracy: 0.6147\n",
            "Epoch 25/25\n",
            "391/391 [==============================] - 32s 83ms/step - loss: 0.8048 - accuracy: 0.7149 - val_loss: 1.1660 - val_accuracy: 0.6104\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1660 - accuracy: 0.6104\n",
            "Test accuracy: 0.6104000210762024\n"
          ]
        }
      ]
    }
  ]
}